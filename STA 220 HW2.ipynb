{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144c6fc5",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "# STA 220 Assignment 2\n",
    "# Jake Tierney, Student ID: 913235409\n",
    "\n",
    "Due __Februrary 9, 2024__ by __11:59pm__. Submit your work by uploading it to Gradescope through Canvas.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Provide your solutions in new cells following each exercise description. Create as many new cells as necessary. Use code cells for your Python scripts and Markdown cells for explanatory text or answers to non-coding questions. Answer all textual questions in complete sentences.\n",
    "2. The use of assistive tools is permitted, but must be indicated. You will be graded on you proficiency in coding. Produce high quality code by adhering to proper programming principles. \n",
    "3. Export the .jpynb as .pdf and submit it on Gradescope in time. To facilitate grading, indicate the area of the solution on the submission. Submissions without indication will be marked down. No late submissions accepted. \n",
    "4. If test cases are given, your solution must be in the same format. \n",
    "5. The total number of points is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe607f59",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__Exercise 1__\n",
    "\n",
    "We will compute the [PageRank](https://en.wikipedia.org/wiki/PageRank) of the articles of the [Sinhala](https://en.wikipedia.org/wiki/Sinhala_language) wikipedia, which is available at [si.wikipedia.org](https://si.wikipedia.org/wiki/%E0%B6%B8%E0%B7%94%E0%B6%BD%E0%B7%8A_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94%E0%B7%80). Additional information of the Sinhala wiki can be found [here](https://meta.wikimedia.org/wiki/List_of_Wikipedias). \n",
    "\n",
    "_Hints: If you don't speak Sinhalese, you might want to learn the wiki logic from the english wikipedia, and translate your findings. Also, caching is highly recommended._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276cd17",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(a)__ Use the special [AllPages](https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%92%E0%B7%81%E0%B7%9A%E0%B7%82:%E0%B7%83%E0%B7%92%E0%B6%BA%E0%B7%85%E0%B7%94_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94) page and understand its logic to retrieve the url of all articles in the sinhalese wikipedia. Make sure to skip redirections.\n",
    "\n",
    "_How many articles are there?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9cbc3e4-f634-42b9-bd4a-93f52b1f3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requisite modules\n",
    "import time\n",
    "import requests as rq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json as js\n",
    "import lxml.html as lx\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c244278-843b-4c5f-8bd5-2fde8d90fb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24227"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 1, Problem a\n",
    "\n",
    "# Defining a function that will return the list of URLs\n",
    "def get_links(initial_url, final_list):\n",
    "\n",
    "    # Set the parameters\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"links\",\n",
    "        \"list\": \"allpages\",\n",
    "        \"aplimit\": \"max\",\n",
    "    }\n",
    "\n",
    "    # Creating a loop that will iterate over all of the AllPages pages\n",
    "    for r in range(1, 150):\n",
    "        \n",
    "        # If/Else statement that uses the initial URL for the first loop, and then an updated URL for subsequent loops\n",
    "        if r == 1:\n",
    "            response = rq.get(initial_url, params = params)\n",
    "        else:\n",
    "            response = rq.get(next_page, params = params)\n",
    "\n",
    "        # Parse the HTML content\n",
    "        data = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all links within the page\n",
    "        links = data.find_all('a')\n",
    "\n",
    "        # Extract link URLs and classes in a list\n",
    "        link_info = []\n",
    "        for link in links:\n",
    "            link_url = link.get('href')\n",
    "            link_class = link.get('class')\n",
    "\n",
    "            # Append link URL and class to the result list\n",
    "            link_info.append({\n",
    "                'url': link_url,\n",
    "                'class': link_class\n",
    "            })\n",
    "\n",
    "        # Set the next page link\n",
    "        for i in range(0, len(link_info)):\n",
    "            if r == 1 and link_info[i] == {'url': 'javascript:print();', 'class': None}:\n",
    "                nav_index = i + 3\n",
    "                next_page = \"https://si.wikipedia.org\" + link_info[nav_index]['url']\n",
    "                break\n",
    "\n",
    "            elif r != 1 and link_info[i] == {'url': 'javascript:print();', 'class': None}:\n",
    "                nav_index = i + 4\n",
    "                next_page = \"https://si.wikipedia.org\" + link_info[nav_index]['url']\n",
    "                break\n",
    "\n",
    "            elif '>මීළඟ පිටුව' not in str(links):\n",
    "                next_page = \"NA\"\n",
    "\n",
    "        # Keep only instances of main article links\n",
    "        for i in range(0, len(link_info)):\n",
    "            if r == 1 and link_info[i] == {'url': 'https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy', 'class': None}:\n",
    "                last_article_index = i - 2\n",
    "                break\n",
    "\n",
    "            elif r != 1 and link_info[i] == {'url': 'https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy', 'class': None}:\n",
    "                last_article_index = i - 3\n",
    "                break\n",
    "\n",
    "        link_info = link_info[nav_index + 1:last_article_index]\n",
    "        \n",
    "        main_articles = []\n",
    "        \n",
    "        for i in link_info:\n",
    "            if i['class'] == None:\n",
    "                main_articles.append(i)\n",
    "\n",
    "        # Creating final list of URLs\n",
    "        for i in main_articles:\n",
    "            wiki_link = \"https://si.wikipedia.org\" + i['url']\n",
    "            final_list.append(wiki_link)\n",
    "\n",
    "        # Creating a break statement if there is no next page\n",
    "        if next_page == \"NA\":\n",
    "            break\n",
    "\n",
    "        # Resting briefly to not make too many requests per minute; aiming for 25 requests per second\n",
    "        time.sleep(0.04)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "# Set the initial URL and final list inputs\n",
    "initial_url = \"https://si.wikipedia.org/wiki/Special:AllPages\"\n",
    "final_list = []\n",
    "\n",
    "# Run the function with these inputs\n",
    "wiki_pages = get_links(initial_url, final_list)\n",
    "\n",
    "# Count the length of the output list of Wiki URLs\n",
    "len(wiki_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb057d5e-e332-4367-b843-d241f2d27360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24227"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that this list contains only unique URLs\n",
    "len(np.unique(np.array(wiki_pages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bef4cb-976e-4fbe-97b6-829adac2e733",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "SOLUTION: There are roughly 24,227 non-redirect article links within the Sinhala Wikipedia AllPages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90213dd",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(b, i)__ Scan all articles in the sinhalese wikipedia and retrieve all links to other articles. Avoid links to special pages, images or the ones that point to another website. Only count the proper article for links that point to a specific section. Use regular expressions to manage these cases. \n",
    "__(ii)__ Make sure to match redirections to their correct destiation article. To this end, find how wikipedia treats redirections and retrieve the true article. _(Help: Try searching for 'uc davis' on en.wikipedia.org')_\n",
    "__(iii)__ Use threading to request all articles and obtain all links to other articles. _(Attention: This takes about thirty minutes!)_\n",
    "\n",
    "\n",
    "_How many links to other articles are there?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a2c11c-97b1-41ae-a605-b701ed44d026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1, Problem b\n",
    "\n",
    "# (i)\n",
    "\n",
    "# Defining the function that will return a list of all wiki URLs referenced in the wiki_pages list\n",
    "def page_urls(input_url_list):\n",
    "\n",
    "    # Initializing the empty article_links list\n",
    "    article_links = []\n",
    "    \n",
    "    #Iterating over every url in the wiki_pages list\n",
    "    for p in input_url_list:\n",
    "\n",
    "        # Making the API request for the given URL\n",
    "        response = rq.get(p)\n",
    "\n",
    "        # Parsing html content\n",
    "        data = BeautifulSoup(response.content, 'html.parser')\n",
    "       \n",
    "        # Finding the body content section by class\n",
    "        content_section = data.find('div', class_='mw-content-ltr mw-parser-output')\n",
    "        \n",
    "        # Checking if content_section is found\n",
    "        if content_section:\n",
    "            # Finding all links within the content section\n",
    "            links = content_section.find_all('a')\n",
    "\n",
    "            # Extracting within-page link URLs and classes\n",
    "            link_info = []\n",
    "            for link in links:\n",
    "                link_url = link.get('href')\n",
    "                link_class = link.get('class')\n",
    "\n",
    "                # Append link URL and class to the result list\n",
    "                link_info.append({\n",
    "                    'url': link_url,\n",
    "                    'class': link_class\n",
    "                })\n",
    "\n",
    "        # Removing any external or special links\n",
    "        for i in range(0, len(link_info)):\n",
    "            crit1 = '/wiki/' in str(link_info[i]['url'])\n",
    "            crit2 = str(link_info[i]['class']) in ['None', \"['mw-redirect']\"]\n",
    "            crit3 = 'https:' not in str(link_info[i]['url'])\n",
    "            crit4 = 'Help:Authority_control' not in str(link_info[i]['url'])\n",
    "            crit5 = 'Special:Search' not in str(link_info[i]['url'])\n",
    "            if crit1 and crit2 and crit3 and crit4 and crit5:\n",
    "                article_links.append(str(link_info[i]))\n",
    "\n",
    "        # Resting briefly to not make too many requests per minute; aiming for 25 requests per second\n",
    "        time.sleep(0.04)\n",
    "\n",
    "    # Creating a list of only the unique article links\n",
    "    article_links = np.unique(np.array(article_links))\n",
    "\n",
    "    return article_links\n",
    "\n",
    "within_article_links = page_urls(wiki_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1bb39e-fdf9-4e69-a32c-65a0709160f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34539"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(within_article_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0618ffc-88bb-4edc-8e0e-f106ceb8a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1, Problem b\n",
    "\n",
    "# (ii)\n",
    "\n",
    "# Splitting the links found in part (i) into redirect and main article lists\n",
    "redirects = []\n",
    "main_articles = []\n",
    "for i in within_article_links:\n",
    "    if 'mw-redirect' in i:\n",
    "        redirects.append(i)\n",
    "    else:\n",
    "        main_articles.append(i)\n",
    "\n",
    "# Finding the destination URL for each of the redirect links\n",
    "destination_urls = []\n",
    "for i in range(0, len(redirects)):\n",
    "    input_url = \"https://si.wikipedia.org\" + redirects[i][redirects[i].find(\"/wiki/\"):redirects[i].find(\"class\") - 4]\n",
    "    response = rq.get(input_url, allow_redirects = False)\n",
    "    data = response.text\n",
    "    destination_url = \"https://si.wikipedia.org\" + data[data.find(\"/wiki/\"):data.find(\"wgCentralAuthMobileDomain\") - 3]\n",
    "    destination_urls.append(destination_url)\n",
    "    time.sleep(0.04)\n",
    "    \n",
    "destination_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "60b11a51-47a5-4234-a32a-84f31bd429c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://si.wikipedia.org/wiki/%E0%B6%87%E0%B6%BD%E0%B7%8A%E0%B7%86%E0%B7%8F_%E0%B6%9A%E0%B7%8A%E0%B7%82%E0%B6%BA%E0%B7%80%E0%B7%93%E0%B6%B8%E0%B7%8A'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 1, Problem b\n",
    "\n",
    "# (ii)\n",
    "\n",
    "input_url = \"https://si.wikipedia.org\" + redirects[6][redirects[6].find(\"/wiki/\"):redirects[6].find(\"class\") - 4]\n",
    "response = rq.get(input_url, allow_redirects = False)\n",
    "data = response.text\n",
    "destination_url = \"https://si.wikipedia.org\" + data[data.find(\"wgInternalRedirectTargetUrl\") + 30:data.find(\"wgCentralAuthMobileDomain\") - 3]\n",
    "destination_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6c6b9b2c-26d3-44db-99db-e3f2beb9707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wiki/%E0%B6%87%E0%B6%BD%E0%B7%8A%E0%B7%86%E0%B7%8F_%E0%B6%9A%E0%B7%8A%E0%B7%82%E0%B6%BA%E0%B7%80%E0%B7%93%E0%B6%B8%E0%B7%8A'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.find(\"wgInternalRedirectTargetUrl\") + 31:data.find(\"wgCentralAuthMobileDomain\") - 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "624efcf2-2e06-410f-9878-96d646a8d633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34539"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redirects) + len(main_articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3938c9c0-a2c3-43be-852a-402a69d2a5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7459"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redirects)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "73adaebe-20f6-44bb-ac1d-5c691c21f8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://si.wikipedia.org/wiki/%E0%B7%83%E0%B6%BB%E0%B7%83%E0%B7%80%E0%B7%92%E0%B6%BA_%E0%B7%84%E0%B7%9C%E0%B6%B3%E0%B6%B8_%E0%B6%A0%E0%B7%92%E0%B6%AD%E0%B7%8A%E2%80%8D%E0%B6%BB%E0%B6%B4%E0%B6%A7%E0%B6%BA'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_url = \"https://si.wikipedia.org\" + redirects[3][redirects[3].find(\"/wiki/\"):redirects[3].find(\"class\") - 4]\n",
    "response = rq.get(input_url, allow_redirects = False)\n",
    "data = response.text\n",
    "destination_url = \"https://si.wikipedia.org\" + data[data.find(\"/wiki/\"):data.find(\"wgCentralAuthMobileDomain\") - 3]\n",
    "destination_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775e365",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(c)__ Compute the transition matrix (see [here](https://en.wikipedia.org/wiki/Google_matrix) and [here](https://www.amsi.org.au/teacher_modules/pdfs/Maths_delivers/Pagerank5.pdf) for step-by-step instructions). Make sure to tread dangling nodes. You may want to use: \n",
    "```\n",
    "from scipy.sparse import csr_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68fe99",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(d, i)__ Set the damping factor to `0.85` and compute the PageRank for each article, using fourty iterations and starting with a vector with equal entries. __(ii)__ Obtain the top ten articles in terms of PageRank, and, retrieving the articles again, find the correponding English article, if available. \n",
    "\n",
    "_Return the corresponding English article titles of the top ten articles from the Sinhalese wikipedia._"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
